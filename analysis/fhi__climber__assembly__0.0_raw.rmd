---
title: 'Assembling Metagenomic Communities (raw code, unifinished version)'
date: "`r format(Sys.time(), '%d %b %Y, %H:%M')`"
output:
  html_document:
    output_file: "shotgun_assembly_raw.html"
    toc: TRUE
  pdf_document: 
    toc: TRUE
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='../documents/shotgun_assembly_raw.html') })
---

**NB:** this is a convenience file, and will probably lag behind the [main version](/documents/shotgun_assembly_raw.html/). It'll be updated periodically (note date above...)

```{bash, eval= FALSE}
# raw code

##   s e t u p   =======================================

# databases for different tools
DB=/data/data/databases
HGR38_BT=/data/databases/hostremoval/Homo_sapiens/Bowtie2/     # <?>
K2REFDIR=/data/databases/Kraken2_GTDB_r89_54k

# our overall data structure
RAW=/data/Food/primary/R0936_redcheese/
WRK=/data/Food/analysis/R0936_redcheese/

# scripts, and slurms for queueing up jobs
MAT=$WRK/Materials
SLURM=$WRK/slurms

# outputs etc
QC=$WRK/1__qc
FQC=$QC/fastqc
FILT=$WRK/2__filt
KDOUT=$WRK/3__knead
KROUT=$WRK/4__krak2

#!# set a filename to test things on
TEST=SAMPLENAME


mkdir $RAW $WRK $FILT $QC $FQC $KDOUT $KROUT

# make script dirs etc.
mkdir $MAT $SLURM

# make dirs for qc outputs
mkdir $FQC/fhi_redch_raw $FQC/fhi_redch_trimm $FQC/multi_raw $FQC/multi_trimm

module load parallel fastqc multiqc kraken2 bowtie2


##   d a t a   &   Q C   =======================================

# then run a fastqc for F and R files, output in the dirs we made above
fastqc -t 4 $RAW/${TEST}* -o $FQC/fhi_redch_raw

# make a report that includes both F and R reads for this sample
multiqc $FQC/fhi_redch_raw -o $FQC/multi_raw


# write a slurm script first
echo '#!/bin/bash

SBATCH –job-name=knead_fastq
SBATCH –output=knead_fastq.txt

SBATCH –ntasks=15
SBATCH –time=15:00

IN=$1
OUT=$2

# time just tells us how long this takes, so we know for next time
# -t is the number of threads (tasks) to use
# curly brackets {} allow us to match either gz or bz2 file extensions
time fastqc -t 15 $IN/*fastq.{bz2,gz} -o $OUT
' > $SLURM/slurm_fqc.sh


# trust slurm
sbatch $SLURM/slurm_fqc.sh $RAW $FQC/fhi_redch_raw

# combine outputs
time multiqc $FQC/fhi_redch_raw -o $FQC/multi_raw

# then copy to local computer (try FileZilla), and open in your browser! 


##   2.  t r i m m   =======================================

echo '>adapter_seq
CTGTCTCTTATACACATCT
>adapter_mate_seq
AGATGTGTATAAGAGACAG
>Transposase_Adap__for_tagmentation_1
TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG
>Transposase_Adap__for_tagmentation_2
GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG
>PCR_primer_index_1
CAAGCAGAAGACGGCATACGAGATNNNNNNNGTCTCGTGGGCTCGG
>PCR_primer_index_2
AATGATACGGCGACCACCGAGATCTACACNNNNNTCGTCGGCAGCGTC
>PCR_primer_index_2_rc
GACGCTGCCGACGANNNNNGTGTAGATCTCGGTGGTCGCCGTATCATT
>PCR_primer_index_1_rc
CCGAGCCCACGAGACNNNNNNNATCTCGTATGCCGTCTTCTGCTTG
>Transposase_Adap__for_tagmentation_2_rc
CTGTCTCTTATACACATCTCCGAGCCCACGAGAC
>Transposase_Adap__for_tagmentation_1_rc
CTGTCTCTTATACACATCTGACGCTGCCGACGA
>adapter_mate_seq_rc
CTGTCTCTTATACACATCT
>adapter_seq_rc
AGATGTGTATAAGAGACAG' > $MAT/fqc_trimmo_ill_ref.fa


trimmomatic PE \
$RAW/${TEST}_R1_001.fastq.gz $RAW/${TEST}_R2_001.fastq.gz \
$FILT/${TEST}_R1_trimm.fastq.gz $FILT/${TEST}_R1_trimm_unpaired.fastq.gz \
$FILT/${TEST}_R2_trimm.fastq.gz $FILT/${TEST}_R2_trimm_unpaired.fastq.gz \
HEADCROP:25 \
CROP:125 \
ILLUMINACLIP:$MAT/fqc_trimmo_ill_ref.fa:2:30:10:5 \
SLIDINGWINDOW:6:15 MINLEN:80 \
-threads 6 > $FILT/trimmo_${TEST}.out


# combine all those different parts!
ls $RAW/*fastq.gz | sed -e 's/.*\/\(.*\)_R._001.*/\1/g' | sort | uniq | > $MAT/samples

# First, we make a slurm script for running Trimmomatic:
echo '#!/bin/bash

#SBATCH –job-name=trimmoRaw
#SBATCH –output=trimmoRaw.txt

#SBATCH –ntasks=4
#SBATCH –time=06:00

# these take the terms given after the scriptname, i.e. "... $i $RAW $FILT $MAT"
SAMPLE=$1
IN=$2
OUT=$3
REFDIR=$4

# trimmomatic - use backslash to separate rows
trimmomatic PE \
$IN/${SAMPLE}_R1_001.fastq.{bz2,gz} \
$IN/${SAMPLE}_R2_001.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R1_trimm.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R1_trimm_unpaired.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R2_trimm.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R2_trimm_unpaired.fastq.{bz2,gz} \
HEADCROP:25 \
CROP:125 \
ILLUMINACLIP:$REFDIR/fqc_trimmo_ill_ref.fa:2:30:10:5 \
SLIDINGWINDOW:6:15 \
MINLEN:80 
-threads 6 > $OUT/trimmo_${SAMPLE}.out #2>&1' > $MAT/slurms/slurm_trimmo.sh


# this command lists all the sample names
cat $MAT/samples

for i in $(cat $MAT/samples);
  do sbatch $MAT/slurms/slurm_trimmo.sh $i $RAW $FILT $MAT;
done

fastqc -t 4 $FILT/*fastq.gz -o $FQC/fhi_redch_filt
multiqc $FQC/fhi_redch_filt -o $FQC/multi_filt


##   3.  d e c o n t a m   =======================================

# download to our raw data folder!

# make a variable for this
BT_DB=/data/Food/analysis/R0936_redcheese/ref

# set this as appropriate
BT_threads=2

# if not already:
module load bowtie2


## make sure you want all of these genomes, or replace with your own! 
echo '#!/bin/bash

cd $1

# download your own genome (more or less) - 1.2 GB, 12min :: from the NCBI browser https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_000001405.40/download?filename=GCF_000001405.40.zip" -H "Accept: application/zip"

# decoy genome - 9 MB
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5ss.fa.gz

# cow genome - 825 MB, 5min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCA_021234555.1/download?filename=GCA_021234555.1.zip" -H "Accept: application/zip"

# sheep genome - 890 MB, 4min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016772045.1/download?filename=GCF_016772045.1.zip" -H "Accept: application/zip"

# goat genome - 930 MB, 3min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_001704415.2/download?filename=GCF_001704415.2.zip" -H "Accept: application/zip"

# chicken genome - 440 MB, 1min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016699485.2/download?filename=GCF_016699485.2.zip" -H "Accept: application/zip"' > /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh

# do check that curl is installed!
curl -h
# if not, try activating it: "module load curl"

# give "run" permission for script to be run
chmod +x /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh
# make a dir for files - just in case RAW isn't added as a variable
mkdir /data/Food/primary/R0936_redcheese/ref
# run the script while pointing at those files
/data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh /data/Food/primary/R0936_redcheese/ref 


  ##
  ##  you will need to extract the genome files (.....fna) all by yourself!
  ##


# compress downloads
parallel gzip {} ::: $RAW/ref/*fna

mkdir $BT_DB

# bowtie2-build [options]* <reference_in> <bt2_base>
time bowtie2-build --t 9 ~/data/ref/bt2_indices/hs37d5ss.fa.gz ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2 > ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2.buildlog

## individual builds:
# decoy.9MB.fna.gz; 19 seconds for decoy, 9 threads, 15GB RAM
# chicken.340MB.fna.gz; 10min, 9 threads, 15GB RAM
# cow.853MB.fna.gz; 28m, 9 threads, 15GB RAM

# build our BT2 database in slurm:
echo '#!/bin/bash

#SBATCH --job-name=bt2DB
#SBATCH --output=bt2DB.txt
#SBATCH --ntasks=10
#SBATCH --time=179:00

INOUT=$1

# do check the names before you run!
time bowtie2-build --large-index --threads 10 \
$INOUT/GCA_021234555.1_ARS-LIC_NZ_Jersey_genomic.fna.gz,\
$INOUT/GCF_000001405.40_GRCh38.p14_genomic.fna.gz,\
$INOUT/GCF_001704415.2_ARS1.2_genomic.fna.gz,\
$INOUT/GCF_016699485.2_bGalGal1.mat.broiler.GRCg7b_genomic.fna.gz,\
$INOUT/GCF_016772045.1_ARS-UI_Ramb_v2.0_genomic.fna.gz,\
$INOUT/hs37d5ss.fna.gz \
$INOUT/mult_ref.bt2 #> $INOUT/mult_ref.buildlog # 2>&1

' > $MAT/slurm_bt2db.sh

# and then run
$MAT/slurm_bt2db.sh $BT_DB


## for one sample:

  ## bowtie2 - align our sequences to ref using bt2.
  #bowtie2 -p $BT_threads -x $BT_DB/mult_ref -1 $FILT/${SAMPLE}_R1.fastq.gz -2 $FILT/${SAMPLE}_R2.fastq.gz -S $KDOUT/${TEST}_bt2_refmapped.sam
  ## feel free to look at the output
  #less $KDOUT/${TEST}_bt2_refmapped.sam
  
  
  ## next with samtools,
  #samtools view -bS $KDOUT/${TEST}_bt2_refmapped.sam > $KDOUT/${TEST}_bt2_refmapped.bam
  
  # F 256 : exclude those mapping to ref
  # f 12 : F and R *both* unmapped
  #samtools view -b -f 12 -F 256 $KDOUT/${TEST}_bt2_refmapped.bam > $KDOUT/${TEST}_bt2_decontamd.bam
  
  # ensure reads are in a sensible order
  #samtools sort -n -m 5G -@ 2 $KDOUT/${TEST}_bt2_decontamd.bam -o $KDOUT/${TEST}_bt2_decontamd_sort.bam
  
  # stream outputs of non-interest to /dev/null
  #samtools fastq -@ 8 $KDOUT/${TEST}_bt2_decontamd_sort.bam -1 ${SAMPLE}_bt2decon_R1.fastq.gz -2 ${SAMPLE}_bt2decon_R2.fastq.gz -0 /dev/null -s /dev/null -n


# build our BT2 database in slurm:
echo '#!/bin/bash

#SBATCH --job-name=bt_deco
#SBATCH --output=bt_deco.txt

# note no time, mem, tasks specified

SAMPLE=$1
IN=$2
OUT=$3
REF=$4
THREADS=$5

## find the contaminants in our sequences, using bt2 + DB
time bowtie2 -p $THREADS \
-x $REF/mult_ref \
-1 $IN/${SAMPLE}_R1.fastq.gz \
-2 $FILT/${SAMPLE}_R2.fastq.gz \
-S $OUT/${SAMPLE}_bt2_refmapped.sam

## change format
time samtools view -bS $OUT/${SAMPLE}_bt2_refmapped.sam > $OUT/${SAMPLE}_bt2_refmapped.bam

# samtools removes reads which match (contaminant) reference genome
time samtools view -b -f 12 -F 256 $OUT/${SAMPLE}_bt2_refmapped.bam > $OUT/${SAMPLE}_bt2_decontamd.bam

# samtools re-roganises reads 
time samtools sort -n -m 5G -@ 2 $OUT/${SAMPLE}_bt2_decontamd.bam -o $OUT/${SAMPLE}_bt2_decontamd_sort.bam

# stream outputs of non-interest to /dev/null
time samtools fastq -@ 8 $OUT/${SAMPLE}_bt2_decontamd_sort.bam -1 ${SAMPLE}_bt2decon_R1.fastq.gz -2 ${SAMPLE}_bt2decon_R2.fastq.gz -0 /dev/null -s /dev/null -n

## opt rm intermediates

' > $MAT/slurm_bt2deco.sh # SAMPLE  IN  OUT REF THREADS

for i in $(cat $MAT/samples | head -1 );

  do sbatch $MAT/slurm_bt2deco.sh $i $FILT $KDOUT $BT_DB 10 ;

done

# should just show your stuff
squeue -u $USER


```

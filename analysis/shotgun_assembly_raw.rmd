---
title: 'Assembling Metagenomic Communities (raw code, unifinished version)'
date: "`r format(Sys.time(), '%d %b %Y, %H:%M')`"
output:
  html_document:
    output_file: "shotgun_assembly_raw.html"
    toc: TRUE
  pdf_document: 
    toc: TRUE
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='../documents/shotgun_assembly_raw.html') })
---

**NB:** this is a convenience file, and will probably lag behind the [main version](/documents/shotgun_assembly_raw.html/). It'll be updated periodically (note date above...)

```{bash, eval= FALSE}
# raw code

##   s e t u p   =======================================

# databases for different tools
DB=/data/data/databases
HGR38_BT=/data/databases/hostremoval/Homo_sapiens/Bowtie2/     # <?>
K2REFDIR=/data/databases/Kraken2_GTDB_r89_54k

# our overall data structure
RAW=/data/Food/primary/R0936_redcheese/
  WRK=/data/Food/analysis/R0936_redcheese/
  
  # scripts, and slurms for queueing up jobs
  MAT=$WRK/Materials
SLURM=$WRK/slurms

# outputs etc
QC=$WRK/1__qc
FQC=$QC/fastqc
FILT=$WRK/2__filt
KDOUT=$WRK/3__knead
KROUT=$WRK/4__krak2

#!# set a filename to test things on
TEST=SAMPLENAME


mkdir $RAW $WRK $FILT $QC $FQC $KDOUT $KROUT

# make script dirs etc.
mkdir $MAT $SLURM

# make dirs for qc outputs
mkdir $FQC/fhi_redch_raw $FQC/fhi_redch_trimm $FQC/multi_raw $FQC/multi_trimm

module load parallel fastqc multiqc kraken2 bowtie2


##   d a t a   &   Q C   =======================================

# then run a fastqc for F and R files, output in the dirs we made above
fastqc -t 4 $RAW/${TEST}* -o $FQC/fhi_redch_raw

# make a report that includes both F and R reads for this sample
multiqc $FQC/fhi_redch_raw -o $FQC/multi_raw


# write a slurm script first
echo '#!/bin/bash

SBATCH –job-name=knead_fastq
SBATCH –output=knead_fastq.txt

SBATCH –ntasks=15
SBATCH –time=15:00

IN=$1
OUT=$2

# time just tells us how long this takes, so we know for next time
# -t is the number of threads (tasks) to use
# curly brackets {} allow us to match either gz or bz2 file extensions
time fastqc -t 15 $IN/*fastq.{bz2,gz} -o $OUT
' > $SLURM/slurm_fqc.sh


# trust slurm
sbatch $SLURM/slurm_fqc.sh $RAW $FQC/fhi_redch_raw

# combine outputs
time multiqc $FQC/fhi_redch_raw -o $FQC/multi_raw

# then copy to local computer (try FileZilla), and open in your browser! 


##   2.  t r i m m   =======================================

echo '>adapter_seq
CTGTCTCTTATACACATCT
>adapter_mate_seq
AGATGTGTATAAGAGACAG
>Transposase_Adap__for_tagmentation_1
TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG
>Transposase_Adap__for_tagmentation_2
GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG
>PCR_primer_index_1
CAAGCAGAAGACGGCATACGAGATNNNNNNNGTCTCGTGGGCTCGG
>PCR_primer_index_2
AATGATACGGCGACCACCGAGATCTACACNNNNNTCGTCGGCAGCGTC
>PCR_primer_index_2_rc
GACGCTGCCGACGANNNNNGTGTAGATCTCGGTGGTCGCCGTATCATT
>PCR_primer_index_1_rc
CCGAGCCCACGAGACNNNNNNNATCTCGTATGCCGTCTTCTGCTTG
>Transposase_Adap__for_tagmentation_2_rc
CTGTCTCTTATACACATCTCCGAGCCCACGAGAC
>Transposase_Adap__for_tagmentation_1_rc
CTGTCTCTTATACACATCTGACGCTGCCGACGA
>adapter_mate_seq_rc
CTGTCTCTTATACACATCT
>adapter_seq_rc
AGATGTGTATAAGAGACAG' > $MAT/fqc_trimmo_ill_ref.fa


trimmomatic PE \
$RAW/${TEST}_R1_001.fastq.gz $RAW/${TEST}_R2_001.fastq.gz \
$FILT/${TEST}_R1_trimm.fastq.gz $FILT/${TEST}_R1_trimm_unpaired.fastq.gz \
$FILT/${TEST}_R2_trimm.fastq.gz $FILT/${TEST}_R2_trimm_unpaired.fastq.gz \
HEADCROP:25 \
CROP:125 \
ILLUMINACLIP:$MAT/fqc_trimmo_ill_ref.fa:2:30:10:5 \
SLIDINGWINDOW:6:15 MINLEN:80 \
-threads 6 > $FILT/trimmo_${TEST}.out


# combine all those different parts!
ls $RAW/*fastq.gz | sed -e 's/.*\/\(.*\)_R._001.*/\1/g' | sort | uniq | > $MAT/samples

# First, we make a slurm script for running Trimmomatic:
echo '#!/bin/bash

#SBATCH –job-name=trimmoRaw
#SBATCH –output=trimmoRaw.txt

#SBATCH –ntasks=4
#SBATCH –time=06:00

# these take the terms given after the scriptname, i.e. "... $i $RAW $FILT $MAT"
SAMPLE=$1
IN=$2
OUT=$3
REFDIR=$4

# trimmomatic - use backslash to separate rows
trimmomatic PE \
$IN/${SAMPLE}_R1_001.fastq.{bz2,gz} \
$IN/${SAMPLE}_R2_001.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R1_trimm.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R1_trimm_unpaired.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R2_trimm.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R2_trimm_unpaired.fastq.{bz2,gz} \
HEADCROP:25 \
CROP:125 \
ILLUMINACLIP:$REFDIR/fqc_trimmo_ill_ref.fa:2:30:10:5 \
SLIDINGWINDOW:6:15 \
MINLEN:80 
-threads 6 > $OUT/trimmo_${SAMPLE}.out #2>&1' > $MAT/slurms/slurm_trimmo.sh


# this command lists all the sample names
cat $MAT/samples

for i in $(cat $MAT/samples);
  do sbatch $MAT/slurms/slurm_trimmo.sh $i $RAW $FILT $MAT;
done

fastqc -t 4 $FILT/*fastq.gz -o $FQC/fhi_redch_filt
multiqc $FQC/fhi_redch_filt -o $FQC/multi_filt


##   3.  d e c o n t a m   =======================================

# make a variable for this
BT_DB=~/data/ref/bt2_indices
mkdir BT_DB

# set this as appropriate
BT_threads=6

# if not already: (not sure if curl is a module..)
module load bowtie2 curl


echo '#!/bin/bash

cd $1

# download your own genome (more or less) - 1.2 GB, 12min :: from the NCBI browser https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_000001405.40/download?filename=GCF_000001405.40.zip" -H "Accept: application/zip"

# decoy genome - 9 MB
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5ss.fa.gz

# cow genome - 825 MB, 5min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCA_021234555.1/download?filename=GCA_021234555.1.zip" -H "Accept: application/zip"

# sheep genome - 890 MB, 4min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016772045.1/download?filename=GCF_016772045.1.zip" -H "Accept: application/zip"

# goat genome - 930 MB, 3min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_001704415.2/download?filename=GCF_001704415.2.zip" -H "Accept: application/zip"

# chicken genome - 440 MB, 1min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016699485.2/download?filename=GCF_016699485.2.zip" -H "Accept: application/zip"' > /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh

# do check that curl is installed!
curl -h
# if not, try activating it: "module load curl"

# give "run" permission for script to be run
chmod +x /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh
# run the script while pointing at those files
/data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh $BT_DB/ 


# compress downloads
parallel gzip {} > $BT_DB/mult_ref.fasta.gz ::: $BT_DB/*fna

# bowtie2-build [options]* <reference_in> <bt2_base>
# test, 19 secs or thereabouts....
time bowtie2-build --t $BT_threads ~/data/ref/bt2_indices/hs37d5ss.fa.gz ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2 > ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2.buildlog

  ## individual builds:
  # decoy.9MB.fna.gz; 19 seconds for decoy, 9 threads, 15GB RAM
  # chicken.340MB.fna.gz; 10min, 9 threads, 15GB RAM
  # cow.853MB.fna.gz; 28m, 9 threads, 15GB RAM

# overall:
time bowtie2-build --large-index --t $BT_threads \
  $BT_DB/mult_ref.fasta.gz \
  $BT_DB/mult_ref.bt2 > \
  $BT_DB/mult_ref.buildlog

```

---
title: 'shotgun metagenomics - sequencing QC - purifying sequences'
author: 'IC / NPV / JFG'
date: "`r format(Sys.time(), '%d %b %Y, %H:%M')`"
output:
  html_document:
    toc: TRUE
  pdf_document: 
    toc: TRUE
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='../documents/3.seqpurity.html') })
---

# 3 - decontaminate data (& check)

Filtering and trimming in the previous steps should have removed nearly all (`r emo::ji("crossed_fingers")`) of the _fake sequences_: low-quality bases that can not be trusted (trimming), as well as artificial sequences introduced by library-preparation and sequencing (filtering).

Next, it is a very good idea to consider what _real sequences_ have been included in our dataset, but do not belong in our analysis. Contaminants could come from: 

  * the experimental environment: host DNA, DNA from surfaces or substrates (foods?) in the environment
  * the method used: i.e. sampling methodology, lab equipment
  * the researcher: sneezing, dribbling, rhinotillexis, colds, coughs, sneezes, or deep PhD-related sighs
  
In some environments (e.g. faeces, sludge, soil), contaminating sequences might comprise a small proportion of the total sequences, and are unlikely to change the "big picture" of the dataset, but could interfere with analysis of specific microbes, or metabolic functions in ways we cannot predict. In other environments (e.g. microbiomes from biopsy, the aeolian microbiome), contaminants can make up the majority of a sample, altering the "big picture" significantly, and making it difficult to draw meaningful conclusions. 

To address this as best we can, we remove contaminants based on a database of reference sequences **appropriate to our study**. We collect these sequences (search for the host, known contaminants, artificial sequences, foodstuffs, etc.), compile them into a database (i.e. a big file that's formatted so that it's easy for computers to search through it quickly), and then ask a program (e.g. the DNA-DNA alignment program, `bowtie2`) to look for, and remove, any sequences that match those contaminants.


## manually removing contaminants - `bowtie2` + reference genomes

We need reference genomes relevant to our experiment. For this tutorial, we consider several possible sources of host contamination:

  * [human genome, assembly `GRCh38.p14, GenBank GCF_000001405.40`](https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/) - contamination from the preparation, extraction, & sampling
  * [the "decoy genome", `hs37d5ss` ('11)](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/README_human_reference_20110707) - a bundle of synthetic, virus-associated, and other miscellaneous sequences that don't map well from/to the human genome ([see more jhere](http://www.cureffi.org/2013/02/01/the-decoy-genome/))
  * [the cow genome, GenBank `GCA_021234555.1` ('21, contig/scaffold N50s: 50M/104M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCA_021234555.1/) - from the source milk
  * [the sheep genome, GenBank `GCA_016772045.1` ('21, contig/scaffold N50s: 43M/101M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_016772045.1/) - from the source milk
  * [the goat genome, GenBank `GCA_001704415.2` ('16,  contig/scaffold N50s: 26M/87M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_001704415.2/) - from the source milk
  * [the chicken genome, genBank `GCA_016699485.1` ('21, contig/scaffold N50s: 19M/91M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_016699485.2/) - host for the chicken gut microbiome

We will first obtain all of these sequences from the NCBI (should/could use ENA...)

```{bash, eval = FALSE}
# download to our raw data folder!

echo '#!/bin/bash

cd $1

# download your own genome (more or less) - 1.2 GB, 12min :: from the NCBI browser https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_000001405.40/download?filename=GCF_000001405.40.zip" -H "Accept: application/zip"

# decoy genome - 9 MB
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5ss.fa.gz

# cow genome - 825 MB, 5min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCA_021234555.1/download?filename=GCA_021234555.1.zip" -H "Accept: application/zip"

# sheep genome - 890 MB, 4min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016772045.1/download?filename=GCF_016772045.1.zip" -H "Accept: application/zip"

# goat genome - 930 MB, 3min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_001704415.2/download?filename=GCF_001704415.2.zip" -H "Accept: application/zip"

# chicken genome - 440 MB, 1min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016699485.2/download?filename=GCF_016699485.2.zip" -H "Accept: application/zip"' > /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh

# do check that curl is installed!
curl -h
# if not, try activating it: "module load curl"

# give "run" permission for script to be run
chmod +x /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh
# make a dir for files - just in case RAW isn't added as a variable
mkdir /data/Food/primary/R0936_redcheese/ref
# run the script while pointing at those files
/data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh /data/Food/primary/R0936_redcheese/ref 


```

Our genomes should be under the directory `/ncbi_dataset/data/<genbank-accession>/`, and will be the largest `file.fna` in there. Extract them to your `$RAW/ref` folder, and then compress the `.fna` to `.fna.gz` with `gzip` to save space. Genome data is fairly dense, so this will/might only compress to about 1/3 the raw size.

```{bash, eval=FALSE}
parallel gzip {} ::: $RAW/ref/*fna
```


It's a lot of data! We then need use `bowtie2` to build reference databases for all of these genomes. `Threads=9` because available locally; one-off leads to poor `slurm` adherence.

```{bash, eval=FALSE}

mkdir ~/data/ref/bt2_indices

# bowtie2-build [options]* <reference_in> <bt2_base>
# 19 seconds for decoy, 9 threads, 15GB RAM
time bowtie2-build --t 9 ~/data/ref/bt2_indices/hs37d5ss.fa.gz ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2 > ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2.buildlog

# 10min for chicken, 9 threads, 15GB RAM
time bowtie2-build --t 9 \
  ~/data/ref/bt2_indices/GCF_016699485.2_bGalGal1.mat.broiler.GRCg7b_genomic.fna.gz \
  ~/data/ref/bt2_indices/broiler_GRCg7b.bt2 > \
  ~/data/ref/bt2_indices/broiler_GRCg7b.bt2.buildlog 

# cow ... 
time bowtie2-build --t 9 \
  ~/data/ref/bt2_indices/GCA_021234555.1_ARS-LIC_NZ_Jersey_genomic.fna.gz \
  ~/data/ref/bt2_indices/jersey_ARS-LIC.bt2 > \
  ~/data/ref/bt2_indices/jersey_ARS-LIC.bt2.buildlog 


```



## Contaminant-removal pipeline - `KneadData`

The [`kneaddata`](https://github.com/biobakery/kneaddata) pipeline was developed to combine both trimming and filtering (`Trimmomatic`), and this contaminant-matching step (`BowTie2`) for processing samples from the human gut. It can also be adapted for other types of sample (e.g. foods, or other hosts, name it). 

```{bash, eval = FALSE}
# re-run trimmomatic disabled using --bypass-trim
module load kneaddata trimmomatic   # needs trimmo, even if you don't

# trial
time kneaddata -t 4 -p 4 --bypass-trim -i $FILT/${TEST}_R1.fastq.gz -i $FILT/${TEST}_R2.fastq.gz -o $KDOUT/${TEST}_2  -db $HGR38_BT --max-memory 45g  --remove-intermediate-output > $KDOUT/knead_${TEST}_2.stout --trimmomatic /install/software/anaconda2.7.a/share/trimmomatic 

## is the output compressed?...
# ; gzip $KDOUT/$TEST/*fastq # 2>&1

```


Super `bash` `slurm` powers. 

```{bash, eval=FALSE}
echo '#!/bin/bash

#SBATCH –-job-name=knead_fastq
#SBATCH –-output=knead_fastq.txt

#SBATCH –-ntasks=4
#SBATCH –-time=40:00

SAMPLE=$1
IN=$2
OUT=$3
REFDIR=$4

##   K N E A D D A T A
# if youre going to change the output names, change for STOUT< GZIP, and FASTQC also
time kneaddata -t 4 -p 4 --bypass-trim -i $IN/${SAMPLE}_R1_trimm.fastq.gz -i $IN/${SAMPLE}_R2_trimm.fastq.gz -o $OUT/${SAMPLE} -db $REFDIR --max-memory 45g > $OUT/knead_${SAMPLE}.stout 2>&1

#gzip $OUT/$SAMPLE/*fastq

' > $MAT/slurm_kd.sh

# demonstrate use of parallel 
ls $FILT/*R[12]_trimm.fastq.gz | sed -e 's/.*\/\(.*\)_R..*/\1/g' | sort | uniq | parallel ls $FILT/{}_R1_trimm.fastq.gz

# actual kneads
ls $FILT/*R[12]_trimm.fastq.gz | sed -e 's/.*\/\(.*\)_R..*/\1/g' | sort | uniq | parallel -j 6 sbatch $MAT/slurms/slurm_kd.sh {} $FILT $KDOUT $HGR38_BT

```


#### fastqc and multiqc

As above, we check the output quality (and can check how many reads etc. make it through).

```{bash, eval = FALSE}
mkdir $FQC/fhi_redch_knead

```


---

# this document isn't finished!

This document is still being written. It still needs the following steps:

  * not really explaining the kneaddata part properly
  * feed clean sequences to `Kraken2`
  * sanitise the `Kraken2` output using `Bracken`
  * start doing the microbial ecology in `R`! But that's a story for another day...
  
---

# Reading / Reference

> kneaddata

> genome data...

> decoy genome

> bowtie2...
---
title: 'Assembling Metagenomic Communities (unifinished version)'
author: 'IC / NPV / JFG'
date: "`r format(Sys.time(), '%d %b %Y, %H:%M')`"
output:
  html_document:
    output_file: "shotgun_assembly.html"
    toc: TRUE
  pdf_document: 
    toc: TRUE
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='../documents/shotgun_assembly.html') })
---

---

```
# todo for jfg

 - utility 
 
 - intro document
 - reference material
 - bibliography
 
 **NB** should unify materials and analysis dirs in setup
``` 


---


# Introduction

#### What this **is**, and **who** it is for:

**This document is not finished**, but covers _**How**_ someone (anyone!) can do the following:

  0. connect through `ssh` and set up a `bash`-based bioinformatics project
  1. collect and check a project's illumina short-read dataset (QC - `FastQC+MultiQC`)
  2. improve the overall quality of their sequencing dataset (QC - `trimmomatic`)
  3. remove contaminant and/or host sequences from the dataset (QC - `kneaddata`)
  4. generate a simple but effective community profile of the microbial community in that dataset (Microbial ecology - `Kraken2+Bracken`)

We provide _minimal_ guidance on how to fit the different steps to the workflow (different parameters, different options or tools). 

This document was originally put together by Ines Calvete (IPLA/CSIC) and Jamie FitzGerald (Teagasc/UCC/UCD) as a roadmap for analysing cheese microbiomes from Asturias, Spain (Oct 2022). It uses material sourced from well-written reference manuals, online communities, and our own mistakes (see *References* section).

We assume a beginner's understanding of the HPC, `ssh`, `bash`, and scripting. See the **Intro document which is not written yet**.  


#### What this document is **not**:

This is not a guide to the _**Why**_ of the methods used here. For that, check the *References* section at the bottom of the page, and the intro document (**Oct 2022: not yet written**).


#### When something goes **wrong**:

Mistakes, and trial-and-error, are part of learning any skill. Bioinformatics involves a particularly large and constant amount of trialing, as well as lots of errors (web search: [**how to read an error**](https://www.baeldung.com/linux/bash-errors)).

Also, one of the best ways to learn quickly (and hopefully make different, exciting mistakes rather than the same ones) is to keep notes of what you do, just like in a lab:

  * write out your objectives
  * date your notes
  * write out what you try to do
  * note down the outcomes
  * write down what goes wrong
  * include the images if you can


#### If you need **help**:

  * in `bash`: for some e.g. `program`, try entering `program --help`, `program -h`, or `man program`..
  * in `R`: for a `function()`, enter `?function` to see that function's help page, or try pressing `tab` after a comma to see other options that `function()` understands.
  * in `R`: for some e.g. `object` (dataframe, matrix, list, etc.) you can try using the `str(object)` function to see its structure, use `head(object)` to see the top 6 lines or rows, `class(object)` to see what sort of thing it is, `dim(object)` to see it's dimensions (`row x columns`; this will be null if it's a list or vector etc., as these only have 1 row and 0 columns, and `1*0 = 0`)
  * try searching for your `error message` online (web search, stackoverflow, biostars, bioconductor, github, huttenhower-biobakery, etc.) - the issue might be simple! If that doesn't work, or you're not sure: 
  * _**Ask someone**_. This is not easy work, and we all know it. People are glad to help, and to show others what they have found out.

---


> Finally, please remember:

>  * when you login to the HPC, you must **always** change to a `node` before you do any work 

>  * you **must** be connected to the Teagasc network/VPN in order to access the  HPC.

>  * take notes, save your data, make backups, brush your teeth.


---  
  
# 0 - setup your environment

It can be helpful to define any frequently used paths / values / variables at the start of your workflow, based on what you want to do. This keeps your workspace visible, easy to refer to, and allows you to reliably change the value throughout your work (e.g. a new project, a new database, etc.). 


#### organising your workspace

A good way to keep track of all your folders/files/values is to use **`$environmental_variables`** - these are temporary shortcuts added to `bash`'s working environment that tell the computer to replace the `$variable` with the file, folder, or value we have linked it to. For example, entering `PLAY=/home/user/Music/playlists` would create a new variable, `$PLAY`: when `$PLAY` is entered, the computer will spot the `$` character, understand that it's a shortcut, and replace that variable (`$PLAY`) with the value assigned to it (`/home/user/Music/playlists`) wherever that variable is found.

We will assign variables to the _folders_ you will use regularly, and for a _sample_ to use for testing things on. Feel free to change these to locations and/or names that make sense to you. Here `$CAPITALLETTERS` help keep these short words visible, but it's not necessary if you don't like shouting. Variables can use letters, numbers, and underscores (more [info on variables](https://ostechnix.com/bash-variables-shell-scripting/)).

We actually use these variables all time! Those created by the user ("user" variables) only last until you close the terminal window, or until a program or `for-loop` finishes (see `$i` in the for-loop section below!) - after this, they will be forgotten. Note that this means you have to enter these variables at each new session (there are clever ways of doing this too). A lot of other variables are created by the system ("system" variables) every time you open a new terminal window: 


```{bash, eval= FALSE}
# $PATH tells the computer all the folders that have programs in them (multiple folder paths)
echo $PATH

# $HOME tells the computer where the user's home directory is (folder paths)
echo $HOME

# $TERM tells the computer what the default command-line interface is  (a value called by other programs)
echo $TERM

# $HISTSIZE tells the computer how many commands to remember (a value)
echo $HISTSIZE

# blah blah blah
```


#### Where does **my** data go?

Thank you for asking!  
**Remember, on the HPC:**

  * **raw data** goes into the `primary` data directory (under your project name on the `hpc`) - here we give it the shortcut variable `$RAW`. This is your 'primary' data source, so do not edit it
  * **output** data goes in the `analysis` folder. We set this variable shortcuts as `$WRK`, and all our output goes inside this dir. 


```{bash, eval = FALSE}
# databases for different tools
DB=/data/data/databases
HGR38_BT=/data/databases/hostremoval/Homo_sapiens/Bowtie2/     # <?>
K2REFDIR=/data/databases/Kraken2_GTDB_r89_54k

# our overall data structure
RAW=/data/Food/primary/R0936_redcheese/
WRK=/data/Food/analysis/R0936_redcheese/

# scripts, and slurms for queueing up jobs
MAT=$WRK/Materials
SLURM=$WRK/slurms

# outputs etc
QC=$WRK/1__qc
FQC=$QC/fastqc
FILT=$WRK/2__filt
KDOUT=$WRK/3__knead
KROUT=$WRK/4__krak2

#!# set a filename to test things on
TEST=SAMPLENAME

```


#### What tools do we need?

We will need to have [`FileZilla`](https://filezilla-project.org/download.php?type=client) (or similar) installed, as well as our `ssh` client (`putty`).

Next, we tell the HPC which programs we'll need, which are parcelled into "modules". If we don't load the modules, our session won't be able to "see" these programs in the `$PATH`. Some programs are always visible, others are loaded through e.g. `conda` and don't need to be mentioned here. 

  * `FastQC` and `MultiQC` for quality control
  * `Kraken2` for metagenomic community reconstruction
  * `Kaiju`, also for metagenomic community reconstruction
  * `GNU-parallel` for case where we can run many jobs in parallel.

```{bash, eval = FALSE}
module load parallel fastqc multiqc kraken2
```


#### Are we ready? 

Finally, we make sure that all the locations for the work we're doing exist! When we start, they won't exist yet (but note how the command looks up the full location for the directory): if `$QC` has not been defined, it will show you the contents of your current directory; if `$QC` _is_ defined, but hasnt been _created_ yet, it will tell you that the folder doesn't exist (which should be true!).
```{bash, eval = FALSE}
ls $QC
```


In our unified data (`$RAW`) and work folders (`$WRK`), we create different folders for each output, using the variables above. It's not a problem if the folder already exists.

```{bash, eval = FALSE}
# make analysis dirs
mkdir $RAW $WRK $FILT $QC $FQC $KDOUT $KROUT

# make script dirs etc.
mkdir $MAT $SLURM

# make dirs for qc outputs
mkdir $FQC/fhi_redch_raw $FQC/fhi_redch_trimm $FQC/multi_raw $FQC/multi_trimm
```


# 1 - get the sequence data (& check)

We copied the data from IPLA-CSIC (or elsewhere!) manually, into the `$RAW` dir, using `FileZilla`. Sometimes we will have to use `scp`, `sFTP`, or `rsync` instead - a web search should show how to do this. 

Next, for our own information, we look at just the `fastq.gz` files, count the number of files, and run `FastQC` and `MultiQC` on them. 

```{bash, eval = FALSE}
# use ls arguments "-lsh" to output it as an informative list
ls -lsh $RAW

# How many files? - use "|" charater to pipe the ls output to wc (wordcount), and count the n of lines (-l or --lines)
ls -lsh $RAW/*fastq.gz | wc -l

# can also easily check a fastq file
ls -lsh $RAW/$TEST*            # file details
zcat $RAW/$TEST* | head -10    # decompress, read and print top 10 lines
less -S $RAW/$TEST*            # read through file, press q to quit
```


##### FastQC and MultiQC

`FastQC` is an amazingly useful tool, for profiling the quality, consistency, and content of your sequence data. It takes samples of each file it is shown (compressed or uncompressed), checks the sequences and returns a `HTML` & `zip` file for each `FASTQ`: the `HTML` is a report, and the `zip` is all the data from that report. Next, `MultiQC` combines all the `FastQC` outputs into one report, summarising all that info in one place. Always check this!

`FastQC` will process all `fastq(.qz)` files that it is passed. `MultiQC` will process all the `FastQC` outputs it finds in a directory, combine them, and place output in the dir specified (`multiqc_report.html` in `-o`). Keep in mind that `FastQC` and `MultiQC` outputs also contain text files of all this data (see the `multiqc_data` dir), if you want to use them for other graphics/reports etc.

#### Check sequence quality, with `FastQC`, on one sample:

```{bash, eval = FALSE}
# then run a fastqc for F and R files, output in the dirs we made above
fastqc -t 4 $RAW/${TEST}* -o $FQC/fhi_redch_raw

# make a report that includes both F and R reads for this sample
multiqc $FQC/fhi_redch_raw -o $FQC/multi_raw
```

Copy the output `multiqc` files to your local computer, and doubleclick to open in your browser. For help reading `FastQC/MultiQC`, there's an [introductory tutorial video](https://www.youtube.com/watch?v=qPbIlO_KWN0) in the top of the `MultiQC` report.


#### Check sequence quality, with `FastQC`, on multiple sample:

We need to be responsible with our use of the HPC. Here we write a script in `slurm`(see the [HPC website](http://hcux400.teagasc.net/) for more info on `slurm`!!) format to queue all these jobs politely into one `FastQC` task, and then summarise using `MultiQC` again. 

We also found out that some `fasstq` files are compressed as `.gz`, and some are compressed at `.bz2`. This means that we need to specify both type fo `fastq` if we want it to work. Best way is to change our command to match both! We show the computer the two different compression types using the curly brackets: `{gz,bz2}`. This allows the computer to accept `*fastq.gz` and `fastq.bz2`:

```{bash, eval = FALSE}
# write a slurm script first
echo '#!/bin/bash

SBATCH –job-name=knead_fastq
SBATCH –output=knead_fastq.txt

SBATCH –ntasks=15
SBATCH –time=15:00

IN=$1
OUT=$2

# time just tells us how long this takes, so we know for next time
# -t is the number of threads (tasks) to use
# curly brackets {} allow us to match either gz or bz2 file extensions
time fastqc -t 15 $IN/*fastq.{bz2,gz} -o $OUT
' > $SLURM/slurm_fqc.sh

# review:
cat $SLURM/slurm_fqc.sh
```

This `slurm` script will give a name, output-log, runtime, and number of threads needed to the `slurm manager`, and use the input and output dirs we give to find `fastq.gz\bz2` files, start FastQC, and place outputs in the $OUT dir. We need to 'give' this `slurm` to `sbatch`:

**Note**: we have noticed that the `{}` brackets do not solve the problem of missing the `bz2` samples. We simply resubmitted the samples as `fastqc -t 4 $RAW/*fastq.bz2 -o $FQC/fhi_redch_raw` - this is not the right thing to do!

```{bash, eval = FALSE}
# trust slurm
sbatch $SLURM/slurm_fqc.sh $RAW $FQC/fhi_redch_raw

# combine outputs
time multiqc $FQC/fhi_redch_raw -o $FQC/multi_raw

# then copy to local computer (try FileZilla), and open in your browser! 
```

Again, open the HTML report in your browser to see what the overall quality is like, and how much cleaning you will need to do.


#### Choosing your sample trimming parameters

With information on all the samples, it is possible (but not necessarily easy) to pick appropriate trimming and quality parameters. 

  * Trimming of the `5'` end removes "artificial" bases at the start of `F+R` reads : check the FastQC or MultiQC outputs to see how long it takes for `Per-Base Sequence Content` to become homogeneous/balanced
  * Trimming of the `3'` end removes bases where the sequencing quality declines gradually (especially the `R` reads)
  * removing adapters removes synthetic sequences that were used to construct the sequencing library, but are not biological

Consider: How do the sequences look? Is quality uniform throughout, or does it vary? What other warnings (in red) do `FQC+MQC` detect?

# 2 - Quality Control (& check)

## First Pass with `Trimmomatic`

Hopefully, there are not too many problems with the data! Nevertheless, we _always_ do a little cleaning of the sequences before we analyse them. There are several things that can go wrong, and we can make the overall dataset better by removing reads or trimming off: 

  * bases with low quality scores (Q score, for illumina: < ~20), meaning that you cannot be sure a base is correct / accurate
  * even if the Q score is good, the start (`5'`) and end (`3'`) of a read are often best removed
  * "adapter", "artefact", or "technical" sequences (the bits of DNA that attach the DNA sample to the machine) can get included by accident
  * "read-through" sequences, where the sequencing gets all the way to the opposite end, and starts reading adapter sequences, or making sequences up
  * other strange problems with operation

We use the program [`Trimmomatic`](http://www.usadellab.org/cms/index.php?page=trimmomatic) to do all of this. This java-based program can do many things to check your quality, but we will focus on:

  * removing adapter sequences
  * trimming the start and ends of reads, based on the `MultiQC` profile we made in section 1.


First, we will use `echo '...' > file.fa` to make a `fasta` file of the known adapter sequences, using references available on the Teagsc HPC, as well as the sequences provided by `Trimmomatic.`

```{bash, eval = FALSE}
echo '>adapter_seq
CTGTCTCTTATACACATCT
>adapter_mate_seq
AGATGTGTATAAGAGACAG
>Transposase_Adap__for_tagmentation_1
TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG
>Transposase_Adap__for_tagmentation_2
GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG
>PCR_primer_index_1
CAAGCAGAAGACGGCATACGAGATNNNNNNNGTCTCGTGGGCTCGG
>PCR_primer_index_2
AATGATACGGCGACCACCGAGATCTACACNNNNNTCGTCGGCAGCGTC
>PCR_primer_index_2_rc
GACGCTGCCGACGANNNNNGTGTAGATCTCGGTGGTCGCCGTATCATT
>PCR_primer_index_1_rc
CCGAGCCCACGAGACNNNNNNNATCTCGTATGCCGTCTTCTGCTTG
>Transposase_Adap__for_tagmentation_2_rc
CTGTCTCTTATACACATCTCCGAGCCCACGAGAC
>Transposase_Adap__for_tagmentation_1_rc
CTGTCTCTTATACACATCTGACGCTGCCGACGA
>adapter_mate_seq_rc
CTGTCTCTTATACACATCT
>adapter_seq_rc
AGATGTGTATAAGAGACAG' > $MAT/fqc_trimmo_ill_ref.fa

# have a look!
less $MAT/fqc_trimmo_ill_ref.fa   # press q to exit

```


#### First Pass, with `Trimmomatic`, on one sample:

To clean the `$TEST` sample, we use these variables:

  * `$RAW` : this was input dir assigned as `/data/Food/primary/../././` 
  * `$FILT` : this was the output dir assigned as `/data/Food/analysis/../././` 
  * `$TEST` :  this was the 'test' sample, assigned as `XXXXXXX`
  * `$MAT` :  this was where we stored scripts, logs, and the `fasta` above assigned as `./././Materials`
    + *note* paths above not completed without reference to Teagasc HPC...  
  
These variables help us to tell `Trimmomatic` the following:

option                            | effect
--------------------------------- | ----------------------------------
`PE`                              | paired-end mode, so examine them together
`$RAW/${TEST}_R1_001.fastq.gz`    | input `F`  reads
`$RAW/${TEST}_R2_001.fastq.gz`    | input `R` reads
`$FILT/${TEST}_R1_trimm.fastq.gz` | output for trimmed `F` reads
`$FILT/${TEST}_R1_trimm_unpaired.fastq.gz` | output for trimmed `F` reads that have lost their `R` mate (see below)
`$FILT/${TEST}_R2_trimm.fastq.gz` | output for trimmed `R` reads
`$FILT/${TEST}_R2_trimm_unpaired.fastq.gz` | output for trimmed `R` reads that have lost their `F` mate (see below) 
`HEADCROP:25`                     | trim the first 25bp from 5' ends  
`CROP:125`                        | trim everything*after** 125bp from 3' ends
`ILLUMINACLIP:$MAT/fqc_trimmo_ill_ref.fa:2:30:10:5` | using the reference `fasta` above, we trim reads with **check parameters on trimming**
`SLIDINGWINDOW:6:15`              | **check parameters on trimming**
`MINLEN:80`                       | after cropping & trimming, sequences must be at least 80bp long to be kept (otherwise delete them)
`-threads 6`                      | use 6 threads (tasks/jobs) in parallel
`> $FILT/trimmo_${TEST}.out`      | use `>` to send error or system messages to a `logfile` so you can check for errors afterwards. 


`Trimmomatic` checks the quality of our sequences, trimming off "bad" sequence bases, and discarding "bad" reads. Most reads will still be paired (have a matching `F` and `R` read). In some cases, the `F` read is discarded (thrown away) but we keep the matching `R` read because it is good quality (or keep `F` and discard `R`). As a result of this, `Trimmomatic` will sort each _sample_ into 4 files:

  1. paired F reads that have a matching R read: `$FILT/${TEST}_R1_trimm.fastq.gz`
  2. paired R reads that have a matching F read: `$FILT/${TEST}_R2_trimm.fastq.gz`
  3. unpaired F reads that don't have a matching R read: `$FILT/${TEST}_R1_trimm_unpaired.fastq.gz`
  4. unpaired R reads that don't have a matching F read: `$FILT/${TEST}_R2_trimm_unpaired.fastq.gz`

  **For now**, we will **not** use the unpaired reads (we ignore them!), as it can lead to complications elsewhere (because of the mix of paired and unpaired reads, which is a problem for some programs). Note also that there are _many_ other options available, to tackle different problems - see the [Trimmomatic manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf) for more.  

<!-- ...but here's the code for it: -->
<!-- ```{bash, eval = FALSE} -->
<!-- ls $FILT/*trimm.fastq.gz | sed -e 's/.*\/\(.*\)_L00._.*/\1/g' | sort | uniq | parallel -j 10 "cat $FILT/{}*_R1_trimm.fastq.gz > $FILT/{}_R1.fastq.gz" -->
<!-- ls $FILT/*trimm.fastq.gz | sed -e 's/.*\/\(.*\)_L00._.*/\1/g' | sort | uniq | parallel -j 10 "cat $FILT/{}*_R2_trimm.fastq.gz > $FILT/{}_R2.fastq.gz" -->
<!-- ``` -->

When it is all put together, it looks like this (the `\` character allows us to start a newline without interrupting the command):

```{bash, eval = FALSE}
trimmomatic PE \
  $RAW/${TEST}_R1_001.fastq.gz $RAW/${TEST}_R2_001.fastq.gz \
  $FILT/${TEST}_R1_trimm.fastq.gz $FILT/${TEST}_R1_trimm_unpaired.fastq.gz \
  $FILT/${TEST}_R2_trimm.fastq.gz $FILT/${TEST}_R2_trimm_unpaired.fastq.gz \
  HEADCROP:25 \
  CROP:125 \
  ILLUMINACLIP:$MAT/fqc_trimmo_ill_ref.fa:2:30:10:5 \
  SLIDINGWINDOW:6:15 MINLEN:80 \
  -threads 6 > $FILT/trimmo_${TEST}.out

# when finished, look at the output:
less $FILT/trimmo_${TEST}.out
```

  
  
#### First Pass, with `Trimmomatic`, on multiple samples:

Because these reads are paired-end, we need to set up a `slurm` script to process each **sample name**, find the `F` and `R` `fastq.gz` files, process them together, and output them to the correct folders. There are a few ways to get the sample names, but because we will need to process each sample name several times, we are going to use a simple, re-usable method:

  * copy the sample names (not the full file names) to a text file, with one sample name per line
  * to process each sample, we then go through the text file, using each sample name in turn.
  
If you are very careful and have a lot of time, you can type all the sample names into a text file by hand. You can also do it quickly using a chain of bash commands joined together by the ['`|`' or `pipe` character](https://ubuntu.com/tutorials/command-line-for-beginners#6-a-bit-of-plumbing):

command | output
------- | ------
`ls $RAW/*fastq.gz` | list all filenames in `$RAW` ending in `fastq.gz`
`sed -e ...` | simplifies the filename using `regex` (`-e` flag; `sed` & `regex` are complex but ++useful - see [intro here](https://www.codesmith.io/blog/understanding-regex))
`s/a/b/g` | `sed`: `s`ubstitute `a` for `b`, everywhere its found (i.e. `g`lobally)
`.*\/\(.*\)_R._001.*` | `sed` - our `a` to be replaced: find text matching `/(...)_R._001`, where '`.`' can be anything, and _keep the bit in brackets_
`\1` | `sed` - our `b` to replace `a`: just paste in the bit found in brackets
`sort` | sorts filenames alphabetically - this is necessary for `uniq`
`uniq` | gives each unique name
`> $MAT/samples` | send filenames to a file in `$MAT` called `samples`

```{bash, eval=FALSE}
# combine all those different parts!
ls $RAW/*fastq.gz | sed -e 's/.*\/\(.*\)_R._001.*/\1/g' | sort | uniq | > $MAT/samples
```

We can then use `cat $MAT/samples` or `less $MAT/samples` to print out all the sample names. We can also feed all these sample names into a `for-loop`, which takes each row (sample name) and places it inside the `for-loop` wherever it finds (e.g.) `$i`  (`$i` is what everyone uses, but it's just a variable like $MAT etc - it could be anything you choose).

First, we make a `slurm` script for running `Trimmomatic`:

```{bash, eval = FALSE}
echo '#!/bin/bash

#SBATCH –job-name=trimmoRaw
#SBATCH –output=trimmoRaw.txt

#SBATCH –ntasks=4
#SBATCH –time=06:00

# these take the terms given after the scriptname, i.e. "... $i $RAW $FILT $MAT"
SAMPLE=$1
IN=$2
OUT=$3
REFDIR=$4

# trimmomatic - use backslash to separate rows
trimmomatic PE \
$IN/${SAMPLE}_R1_001.fastq.{bz2,gz} \
$IN/${SAMPLE}_R2_001.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R1_trimm.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R1_trimm_unpaired.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R2_trimm.fastq.{bz2,gz} \
$OUT/${SAMPLE}_R2_trimm_unpaired.fastq.{bz2,gz} \
HEADCROP:25 \
CROP:125 \
ILLUMINACLIP:$REFDIR/fqc_trimmo_ill_ref.fa:2:30:10:5 \
SLIDINGWINDOW:6:15 \
MINLEN:80 
-threads 6 > $OUT/trimmo_${SAMPLE}.out #2>&1' > $MAT/slurms/slurm_trimmo.sh

```


Then we make a simple [`for-loop` (see this link)](https://linuxize.com/post/bash-for-loop/) that repeats the command `sbatch $MAT/slurms/slurm_trimmo.sh $i $RAW $FILT $MAT` for every possible value of `$i` (all the sample names). In this way, we can use 6 threads to process _one sample after another_, with the job using 6 threads as we specified above (i.e. process samples *in serial* rather than *in parallel*).

```{bash, eval=FALSE}
# this command lists all the sample names
cat $MAT/samples

# we can put that command $(in backets);, and feed it into the for-loop like this:
for i in $(cat $MAT/samples);

  # and it will repeat this command for all the different values of $i, below:
  do sbatch $MAT/slurms/slurm_trimmo.sh $i $RAW $FILT $MAT;

# close the for-loop
done
 
```


#### fastqc and multiqc

Again, we check the `Trimmomatic` output with `FastQC/MultiQC`, copying the multiqc_report to our computer, and seeing if there's much improvement.

```{bash, eval = FALSE}
fastqc -t 4 $FILT/*fastq.gz -o $FQC/fhi_redch_filt
multiqc $FQC/fhi_redch_filt -o $FQC/multi_filt
```


## Second Pass etc.

It is hard to know when the quality is 'good enough'. In general, we should keep above `Q30`, but this is not an indicator of other things like adapter contamination. Often, a later step will not work, and it can be necessary to be stricter in our cleaning, or try different approaches etc. Be clear with yourself, and with other people, about what pre-processing steps you are taking.  

Sometimes, it might be necessary to use `Trimmomatic` more than once before the sequence is 'clean'. In general it is easier to program, easier to organise, and easier to reproduce a metagenomics analysis when all sequence cleaning steps are carried out in one go. Again, **keep notes of what you have done** - this makes remembering what happened much easier.  


# 3 - decontaminate data (& check)

Filtering and trimming in the previous steps should have removed nearly all (`r emo::ji("crossed_fingers")`) of the _fake sequences_: low-quality bases that can not be trusted (trimming), as well as artificial sequences introduced by library-preparation and sequencing (filtering).
%>% 
Next, it is a very good idea to consider what _real sequences_ have been included in our dataset, but do not belong in our analysis. Contaminants could come from: 

  * the experimental environment: host DNA, DNA from surfaces or substrates (foods?) in the environment
  * the method used: i.e. sampling methodology, lab equipment
  * the researcher: sneezing, dribbling, rhinotillexis, colds, coughs, sneezes, or deep PhD-related sighs
  
In some environments (e.g. faeces, sludge, soil), contaminating sequences might comprise a small proportion of the total sequences, and are unlikely to change the "big picture" of the dataset, but could interfere with analysis of specific microbes, or metabolic functions in ways we cannot predict. In other environments (e.g. microbiomes from biopsy, the aeolian microbiome), contaminants can make up the majority of a sample, altering the "big picture" significantly, and making it difficult to draw meaningful conclusions. 

To address this as best we can, we remove contaminants based on a database of reference sequences **appropriate to our study**. We collect these sequences (search for the host, known contaminants, artificial sequences, foodstuffs, etc.), compile them into a database (i.e. a big file that's formatted so that it's easy for computers to search through it quickly), and then ask a program (e.g. the DNA-DNA alignment program, `bowtie2`) to look for, and remove, any sequences that match those contaminants.


## manually removing contaminants - `bowtie2` + reference genomes

We need reference genomes relevant to our experiment. For this tutorial, we consider several possible sources of host contamination:

  * [human genome, assembly `GRCh38.p14, GenBank GCF_000001405.40`](https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/) - contamination from the preparation, extraction, & sampling
  * [the "decoy genome", `hs37d5ss` ('11)](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/README_human_reference_20110707) - a bundle of synthetic, virus-associated, and other miscellaneous sequences that don't map well from/to the human genome ([see more jhere](http://www.cureffi.org/2013/02/01/the-decoy-genome/))
  * [the cow genome, GenBank `GCA_021234555.1` ('21, contig/scaffold N50s: 50M/104M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCA_021234555.1/) - from the source milk
  * [the sheep genome, GenBank `GCA_016772045.1` ('21, contig/scaffold N50s: 43M/101M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_016772045.1/) - from the source milk
  * [the goat genome, GenBank `GCA_001704415.2` ('16,  contig/scaffold N50s: 26M/87M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_001704415.2/) - from the source milk
  * [the chicken genome, genBank `GCA_016699485.1` ('21, contig/scaffold N50s: 19M/91M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_016699485.2/) - host for the chicken gut microbiome

We will first obtain all of these sequences from the NCBI (should/could use ENA...)

```{bash, eval = FALSE}
# download to our raw data folder!

echo '#!/bin/bash

cd $1

# download your own genome (more or less) - 1.2 GB, 12min :: from the NCBI browser https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_000001405.40/download?filename=GCF_000001405.40.zip" -H "Accept: application/zip"

# decoy genome - 9 MB
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5ss.fa.gz

# cow genome - 825 MB, 5min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCA_021234555.1/download?filename=GCA_021234555.1.zip" -H "Accept: application/zip"

# sheep genome - 890 MB, 4min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016772045.1/download?filename=GCF_016772045.1.zip" -H "Accept: application/zip"

# goat genome - 930 MB, 3min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_001704415.2/download?filename=GCF_001704415.2.zip" -H "Accept: application/zip"

# chicken genome - 440 MB, 1min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016699485.2/download?filename=GCF_016699485.2.zip" -H "Accept: application/zip"' > /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh

# do check that curl is installed!
curl -h
# if not, try activating it: "module load curl"

# give "run" permission for script to be run
chmod +x /data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh
# make a dir for files - just in case RAW isn't added as a variable
mkdir /data/Food/primary/R0936_redcheese/ref
# run the script while pointing at those files
/data/Food/analysis/R0936_redcheese/trimmo_genomes_dl.sh /data/Food/primary/R0936_redcheese/ref 


```

Our genomes should be under the directory `/ncbi_dataset/data/<genbank-accession>/`, and will be the largest `file.fna` in there. Extract them to your `$RAW/ref` folder, and then compress the `.fna` to `.fna.gz` with `gzip` to save space. Genome data is fairly dense, so this will/might only compress to about 1/3 the raw size.

```{bash, eval=FALSE}
parallel gzip {} ::: $RAW/ref/*fna
```


It's a lot of data! We then need use `bowtie2` to build reference databases for all of these genomes. `Threads=9` because available locally; one-off leads to poor `slurm` adherence.

```{bash, eval=FALSE}
mkdir ~/data/ref/bt2_indices

# bowtie2-build [options]* <reference_in> <bt2_base>
# 19 seconds for decoy, 9 threads, 15GB RAM
time bowtie2-build --t 9 ~/data/ref/bt2_indices/hs37d5ss.fa.gz ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2 > ~/data/ref/bt2_indices/decoy_hs37d5ss.bt2.buildlog

# 10min for chicken, 9 threads, 15GB RAM
time bowtie2-build --t 9 \
  ~/data/ref/bt2_indices/GCF_016699485.2_bGalGal1.mat.broiler.GRCg7b_genomic.fna.gz \
  ~/data/ref/bt2_indices/broiler_GRCg7b.bt2 > \
  ~/data/ref/bt2_indices/broiler_GRCg7b.bt2.buildlog 

# cow ... 
time bowtie2-build --t 9 \
  ~/data/ref/bt2_indices/GCA_021234555.1_ARS-LIC_NZ_Jersey_genomic.fna.gz \
  ~/data/ref/bt2_indices/jersey_ARS-LIC.bt2 > \
  ~/data/ref/bt2_indices/jersey_ARS-LIC.bt2.buildlog 

```


## Contaminant-removal pipeline - `KneadData`

The [`kneaddata`](https://github.com/biobakery/kneaddata) pipeline was developed to combine both trimming and filtering (`Trimmomatic`), and this contaminant-matching step (`BowTie2`) for processing samples from the human gut. It can also be adapted for other types of sample (e.g. foods, or other hosts). 

```{bash, eval = FALSE}
# re-run trimmomatic disabled using --bypass-trim
module load kneaddata trimmomatic   # needs trimmo, even if you don't

# trial
time kneaddata -t 4 -p 4 --bypass-trim -i $FILT/${TEST}_R1.fastq.gz -i $FILT/${TEST}_R2.fastq.gz -o $KDOUT/${TEST}_2  -db $HGR38_BT --max-memory 45g  --remove-intermediate-output > $KDOUT/knead_${TEST}_2.stout --trimmomatic /install/software/anaconda2.7.a/share/trimmomatic 

## is the output compressed?...
# ; gzip $KDOUT/$TEST/*fastq # 2>&1

```


Super `bash` `slurm` powers. 

```{bash, eval=FALSE}
echo '#!/bin/bash

#SBATCH –-job-name=knead_fastq
#SBATCH –-output=knead_fastq.txt

#SBATCH –-ntasks=4
#SBATCH –-time=40:00

SAMPLE=$1
IN=$2
OUT=$3
REFDIR=$4

##   K N E A D D A T A
# if youre going to change the output names, change for STOUT< GZIP, and FASTQC also
time kneaddata -t 4 -p 4 --bypass-trim -i $IN/${SAMPLE}_R1_trimm.fastq.gz -i $IN/${SAMPLE}_R2_trimm.fastq.gz -o $OUT/${SAMPLE} -db $REFDIR --max-memory 45g > $OUT/knead_${SAMPLE}.stout 2>&1

#gzip $OUT/$SAMPLE/*fastq

' > $MAT/slurm_kd.sh

# actual kneads
ls $FILT/*R[12]_trimm.fastq.gz | sed -e 's/.*\/\(.*\)_R..*/\1/g' | sort | uniq | parallel -j 6 sbatch $MAT/slurms/slurm_kd.sh {} $FILT $KDOUT $HGR38_BT

```


#### fastqc and multiqc

As above, we check the output quality (and can check how many reads etc. make it through).

```{bash, eval = FALSE}
mkdir $FQC/fhi_redch_knead

```


<!-- # 4 microbial community profiling -->
<!-- ```{bash, eval = FALSE} -->

<!-- usearch -fastq_join $KDOUT/$SAMPLE/${SAMPLE}_R1_kneaddata_paired_R1.fastq.gz -reverse $KDOUT/$SAMPLE/${SAMPLE}_R1_paired_R2.fastq.gz -fastaout $KDOUT/${SAMPLE}_kneaddata_R1R2.fastq.gz -->

<!-- # --------- -->

<!-- # test -->
<!-- K2SAMPLE=SC145C1-NS_S86 -->
<!-- K2IN=$KDOUT -->
<!-- K2OUT=$KROUT -->
<!-- K2REFDIR=/data/databases/Kraken2_GTDB_r89_54k -->

<!-- # recall - first run requires a load to shared mem step, which takes time  -  to do this, possibly omit the memory mapping step. GOing to omit entirely as mem not an issue, yet -->
<!-- cat $K2IN/${K2SAMPLE}/${K2SAMPLE}_R1_kneaddata_Homo*.fastq.gz > $K2IN/${K2SAMPLE}/${K2SAMPLE}_contam.fastq.gz & -->
<!-- cat $K2IN/${K2SAMPLE}/${K2SAMPLE}_R1_kneaddata_unmatched_[12].fastq.gz > $K2IN/${K2SAMPLE}/${K2SAMPLE}_orphan.fastq.gz & -->
<!-- # THREE MINUTES! only 18% classified though.  | 0.5 conf: 2m, 2% class | 0.15 cutoff - 3m, 13% ID'd -->
<!-- time kraken2 --db $K2REFDIR  $K2IN/${K2SAMPLE}/${K2SAMPLE}_R1_kneaddata_paired_1.fastq.gz $K2IN/${K2SAMPLE}/${K2SAMPLE}_R1_kneaddata_paired_2.fastq.gz --paired --confidence 0.15 --gzip-compressed --threads 5 --report-zero-counts --report $K2OUT/${K2SAMPLE}_test_kraken_report# --unclassified-out $K2OUT/${K2SAMPLE}_test_kraken_unclass# --output $K2OUT/${K2SAMPLE}_test_kraken_output# -->
<!-- # TWO-POINT-FIVE MINUTES! 27% identified  | conf 0.5: 1m30, 2% identifiedi | 3 MIN 30 - 15% classified -->
<!-- time kraken2 --db $K2REFDIR  $K2IN/${K2SAMPLE}/${K2SAMPLE}_orphan.fastq.gz --gzip-compressed --confidence 0.15 --threads 5 --report-zero-counts --report $K2OUT/${K2SAMPLE}_test_orphan_kraken_report --unclassified-out $K2OUT/${K2SAMPLE}_test_orphan_kraken_unclass --output $K2OUT/${K2SAMPLE}_test_orphan_kraken_output -->
<!-- # THREE MIN FORTY! 12% sequences identified, highly worrying. -->
<!-- time kraken2 --db $K2REFDIR  $K2IN/${K2SAMPLE}/${K2SAMPLE}_contam.fastq.gz --gzip-compressed --confidence 0.15 --threads 5 --report-zero-counts --report $K2OUT/${K2SAMPLE}_test_contam_kraken_report --unclassified-out $K2OUT/${K2SAMPLE}_test_contam_kraken_unclass --output $K2OUT/${K2SAMPLE}_test_contam_kraken_output -->

<!-- # need to specify id cutoff -->

<!-- # for ROUND 1, paired-mode only is fine, as some disagreement about handling btw forum and manual - possibly has been updated in latter. -->
<!-- # can remember why we included the zero counts...but we did, intentionally -->

<!-- echo '#!/bin/bash -->

<!-- #SBATCH –-job-name=knead_fastq -->
<!-- #SBATCH –-output=knead_fastq.txt -->

<!-- #SBATCH –-ntasks=5 -->
<!-- #SBATCH –-time=40:00 -->

<!-- SAMPLE=$1 -->
<!-- IN=$2 -->
<!-- OUT=$3 -->
<!-- REFDIR=$4 -->

<!-- time kraken2 --db $REFDIR  $K2IN/${K2SAMPLE}/${K2SAMPLE}_R1_kneaddata_paired_1.fastq.gz $K2IN/${K2SAMPLE}/${K2SAMPLE}_R1_kneaddata_paired_2.fastq.gz --paired --confidence 0.15 --gzip-compressed --threads 5 --report-zero-counts --use-mpa-style --report $K2OUT/${K2SAMPLE}_kraken_report# --unclassified-out $K2OUT/${K2SAMPLE}_kraken_unclass# --output $K2OUT/${K2SAMPLE}_kraken_output# -->

<!-- ## -->
<!-- ## here, should cat and grab the orphan output -->
<!-- ## -->

<!-- scp $OUT/${SAMPLE}* jamie@143.239.204.169:/mnt/yabba/yoda_backup/home/ClaessonFTP/jamie/ -->

<!-- ## within dir, do fastqc -o $QCDIR/ -t 4 $OUT/${SAMPLE}/*.fastq.gz -->


<!-- ' > $MAT/slurm_krak2.sh -->

<!-- ls $FILT/*R[12].fastq.gz | sed -e 's/.*\/\(.*\)_R..*/\1/g' | sort | uniq | head -20 | tail -1 | parallel -j 8 sbatch $MAT/slurms/slurm_krak2.sh {} $KDOUT $KROUT $K2REFDIR -->
<!-- ls $FILT/*R[12].fastq.gz | sed -e 's/.*\/\(.*\)_R..*/\1/g' | sort | uniq | head -20 | tail -1 | parallel -j 8 $MAT/slurms/bash_krak2.sh {} $KDOUT $KROUT $K2REFDIR -->


<!-- # lifeboat protocol -->
<!-- # K2SAMPLE=SC145C1-NS_S86 -->
<!-- K2IN=/claesson/jamie/fhi_redch -->
<!-- K2OUT=/claesson/jamie/fhi_redch/krak2 -->
<!-- K2REFDIR=/claesson/jamie/ref/kraken2_stnd -->


<!-- # << < < < need to add step here to start with one sample, removing --mem-map, and updating dirs/ -->
<!-- ## remove --mpa-style as interferes with Bracken -->
<!-- ls /claesson/jamie/fhi_redch/*[12].fastq.gz | sed -e 's/.*\/\(.*\)_R..*/\1/g' | sort | uniq | parallel -j 6 time kraken2 --db $K2REFDIR  $K2IN/{}_R1_kneaddata_paired_1.fastq.gz $K2IN/{}_R1_kneaddata_paired_2.fastq.gz --memory-mapping --paired --confidence 0.15 --gzip-compressed --threads 5 --report-zero-counts --report $K2OUT/{}_kraken_report# --unclassified-out $K2OUT/{}_kraken_unclass# --output $K2OUT/{}_kraken_output# >> ~/para_krak_fhi_redch.stout 2>&1 -->

<!-- # this'll put out all the seq too. if you only want the report, send your output/unclassified ..away -->
<!-- # try catch output, or anything else - because where does it go? -->

<!-- ## pre-run very possibly not necessary with --memorymapping opt -->



<!-- ##   B R A C K E N   B E A T I N G       ## =================== -->

<!-- brthreads=10 -->
<!-- b=$DAN/3_b2  ; mkdir $b -->
<!-- binst=~/BioApps -->

<!-- # K2SAMPLE=SC145C1-NS_S86 -->
<!-- WRK=/claesson/jamie/fhi_redch -->
<!-- K2IN=/claesson/jamie/fhi_redch/knead -->
<!-- K2OUT=/claesson/jamie/fhi_redch/krak2 -->
<!-- K2REFDIR=/claesson/jamie/ref/kraken2_stnd -->

<!-- BRK=$WRK/brack ; mkdir $BRK -->

<!-- ## build data base first, using length 80bp 35kmers, takes an hour on 25 threads -->

<!-- ## fast as immediate  -  run bracken on the kraken_report  -  note no threads arg! -->
<!-- # -r = read lengths (taken as the starting read length, we're at about 125 by now...) -->
<!-- # -l = Level, def=S ; (K, P, C, O, F, G, S ) -->
<!-- # -t = threshold, def = 10 ; level below which a taxo-branch on the kraken tree will be dropped from further assignment of abundances -->
<!-- ls $K2OUT/* | sed -r 's/.*\/(.*)_kraken.*/\1/g' | sort | uniq | \ -->
<!-- parallel -j 3  \ -->
<!-- python ~/BioApps/Bracken/src/est_abundance.py -i $K2OUT/*{}*report -k $K2REFDIR/database80mers.kmer_distrib -l S -t 10 -o $BRK/{}.bracken \ -->
<!-- > $BRK/bracken_output_82.stout -->
<!-- # bracken -d $DB/kraken2_stnd -i $KRK/*{}_*report -o $b/{}_bracken_report.txt -r 150 -l S -t 10 > $b/bracken.stout -->

<!-- ## combine reports to one monolith.tsv -->
<!-- python2 ~/BioApps/Bracken/analysis_scripts/combine_bracken_outputs.py --files $BRK/*bracken -o fhi_redch_krakenStnd_bracken_combo -->

<!-- ## convert to MPA-style report outputs, in order to get taxonomic tree (again, surely a better way than this) -->
<!-- parallel python2 ~/BioApps/Bracken/src/kreport2mpa.py -r {} -o $BRK/{.}.mpa ::: $K2OUT/*report* > $BRK/fhi_redch_bragglers -->


<!-- # # catch done/undone -->
<!-- #   find $KOUT/*gz | sed -r 's/.*[\/_]([0-9]{4}[AB]?)[\/_].*/\1/g' | sort | uniq > $DAN/Materials/temp2 -->
<!-- #   find $KRK/*report | sed -r 's/.*[\/_]([0-9]{4}[AB]?)[\/_].*/\1/g' | sort | uniq > $DAN/Materials/temp3_k -->
<!-- #   comm -23 $DAN/Materials/temp2 $DAN/Materials/temp3_k > $DAN/Materials/krk_offenders.txt -->
<!-- #   # include switch for outputting zero-reads - for tabulation -->
<!-- # -->

<!-- ##   T A X O N O M Y   -   glom from mpa output!   ## ======================================== -->

<!-- ## own funciton gettax_kraken : hacky, ++slow, and only gets species name. Avoid. -->

<!-- ## get all the S level assignments; trim counts ; sort; uniq -->
<!-- grep -h '|s_' $BRK/*mpa | cut -f 1 > $BRK/bracken_82_taxa.temp -->
<!-- sort $BRK/bracken_82_taxa.temp | uniq > $BRK/bracken_82_taxa.temp2 -->
<!-- sed 's/|/\t/g' $BRK/bracken_82_taxa.temp2 > $BRK/fhi_redch_krakenStnd_bracken_combo_taxa_82 -->

<!-- ## send home -->
<!-- scp ./bracken_545_*[combo,taxa] $DSK -->

<!-- # consider total taxonomy, but prob doesn't tell us anything re: missing levels -->
<!-- cat $b/bracken_reports_545/*mpa | cut -f 1 > $b/bracken_545_total_tax.temp -->
<!-- sort  $b/bracken_545_total_tax.temp | uniq > $b/bracken_545_total_tax.temp2 -->


<!-- ``` -->


<!-- # Checking your output -->
<!-- ```{bash, eval = FALSE} -->
<!-- ``` -->


---

# this document isn't finished!

This document is still being written. It still needs the following steps:

  * feed clean sequences to `Kraken2`
  * sanitise the `Kraken2` output using `Bracken`
  * start doing the microbial ecology in `R`! But that's a story for another day...
  
---

# Reading / Reference

> [Linux command line for beginners](https://ubuntu.com/tutorials/command-line-for-beginners) - quite quite good!

> [MultiQC site + support](https://multiqc.info)

> [Trimmomatic site](http://www.usadellab.org/cms/index.php?page=trimmomatic)

> [Trimmomatic manual](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf)

> [`for-loop`s - first step in `bash` superpowers](https://linuxize.com/post/bash-for-loop/)

> [How to read an error](https://www.baeldung.com/linux/bash-errors)

> [info on variables](https://ostechnix.com/bash-variables-shell-scripting/)
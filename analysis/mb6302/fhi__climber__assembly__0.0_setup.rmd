---
title: 'Microbiome Analysis: starting & setup `r emo::ji("person_climbing")`'
author: 'jfg'
date: "`r format(Sys.time(), '%d %b %Y, %H:%M')`"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    number_sections: FALSE
  pdf_document: 
    toc: TRUE
    toc_depth: 3
    number_sections: FALSE
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='../../documents/mb6302__setup.html') })
---

---  
  
# 0 - setup your environment

It can be helpful to define any frequently used paths / values / variables at the start of your workflow, based on what you want to do. This keeps your workspace visible, easy to refer to, and allows you to reliably change the value throughout your work (e.g. a new project, a new database, etc.). 


#### organising your workspace

A good way to keep track of all your folders/files/values is to use **`$environmental_variables`** - these are temporary shortcuts added to `bash`'s working environment that tell the computer to replace the `$variable` with the file, folder, or value we have linked it to. For example, entering `PLAY=/home/user/Music/playlists` would create a new variable, `$PLAY`: when `$PLAY` is entered, the computer will spot the `$` character, understand that it's a shortcut, and replace that variable (`$PLAY`) with the value assigned to it (`/home/user/Music/playlists`) wherever that variable is found.

We will assign variables to the _folders_ you will use regularly, and for a _sample_ to use for testing things on. Feel free to change these to locations and/or names that make sense to you. Here `$CAPITALLETTERS` help keep these short words visible, but it's not necessary if you don't like shouting. Variables can use letters, numbers, and underscores (more [info on variables](https://ostechnix.com/bash-variables-shell-scripting/)).

We actually use these variables all time! Those created by the user ("user" variables) only last until you close the terminal window, or until a program or `for-loop` finishes (see `$i` in the for-loop section below!) - after this, they will be forgotten. Note that this means you have to enter these variables at each new session (there are clever ways of doing this too). A lot of other variables are created by the system ("system" variables) every time you open a new terminal window: 


```{bash, eval= FALSE}
# $PATH tells the computer all the folders that have programs in them (multiple folder paths)
echo $PATH

# $HOME tells the computer where the user's home directory is (folder paths)
echo $HOME

# $TERM tells the computer what the default command-line interface is  (a value called by other programs)
echo $TERM

# $HISTSIZE tells the computer how many commands to remember (a value)
echo $HISTSIZE

# blah blah blah
```


#### where does **my** data go?

Thank you for asking! **Remember, on the HPC:**

  * **raw data** goes into the `primary` data directory (under your project name on the `hpc`) - here we give it the shortcut variable `$RAW`. This is your 'primary' data source, so do not edit it
  * **output** data goes in the `analysis` folder. We set this variable shortcuts as `$WRK`, and all our output goes inside this dir. 


<!-- INES=/data/Food/analysis/R0936_redcheese -->
WRK=/data/Food/analysis/R0602_microsupport/r0936


```{bash, eval = FALSE}
# databases for different tools
DB=/data/databases                                            # HPC only
HGR38_BT=/data/databases/hostremoval/Homo_sapiens/Bowtie2     # HPC only
K2REFDIR=/data/databases/kraken2                              # HPC only
BT_DB=/data/Food/analysis/R0936_redcheese/ref

# our overall data structure
RAW=/data/Food/primary/R0936_redcheese
WRK=/data/Food/analysis/R0936_redcheese

# scripts, and slurms for queueing up jobs
MAT=$WRK/Materials

# outputs etc
QC=$WRK/1__qc
FQC=$QC/fastqc
FILT=$WRK/2__filt
KDOUT=$WRK/3__knead
KROUT=$WRK/4__krak2

#!# set a sample-name to test things on
TEST=Q4T8

```


#### what tools do we need?

We will want to have [`FileZilla`](https://filezilla-project.org/download.php?type=client) (or similar) installed, as well as our `ssh` client ([`putty`](https://putty.org)).

Next, we tell the HPC which programs we'll need for this work, which the HPC has parcelled into "modules". If we don't load the modules, our session won't be able to "see" these programs in the `$PATH` (list of places the computer looks to find programs) Some programs are always visible (e.g. commands like `ls`), others are loaded through e.g. `conda` (e.g. `R`...), and don't need to be mentioned here. 

  * `FastQC` and `MultiQC` for quality control
  * `Kraken2` for metagenomic community reconstruction
  * `Kaiju`, also for metagenomic community reconstruction
  * `GNU-parallel` for case where we can run many jobs in parallel.

```{bash, eval = FALSE}
# this is a list of all programs explicitly required in this workflow
module load parallel fastqc multiqc trimmomatic bowtie2 samtools/1.10 kraken2/2.1.1 braken
```


#### are we ready? 

Finally, we make sure that all the locations for the work we're doing exist! When we start, they won't exist yet (but note how the command looks up the full location for the directory): if `$QC` has not been defined, it will show you the contents of your current directory; if `$QC` _is_ defined, but hasn't been _created_ yet, it will tell you that the folder doesn't exist (which should be true!).
```{bash, eval = FALSE}
ls $QC
```


In our unified data (`$RAW`) and work folders (`$WRK`), we create different folders for each output, using the variables above. It's not a problem if the folder already exists.

```{bash, eval = FALSE}
# make analysis dirs
mkdir $BT_DB $RAW $WRK $FILT $QC $FQC $KDOUT $KROUT

# make script dirs etc.
mkdir $MAT

# make dirs for qc outputs
mkdir $FQC/fhi_redch_raw $FQC/fhi_redch_filt $FQC/fhi_redch_raw_multi $FQC/fhi_redch_filt_multi
```


## add link to next section


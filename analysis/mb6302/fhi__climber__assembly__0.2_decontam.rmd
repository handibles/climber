---
title: 'Microbiome Analysis: sanitising sequences `r emo::ji("person_climbing")`'
author: 'jfg'
date: "`r format(Sys.time(), '%d %b %Y, %H:%M')`"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
    toc_float: TRUE
    number_sections: FALSE
  pdf_document: 
    toc: TRUE
    toc_depth: 3
    number_sections: FALSE
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding=encoding, output_file='../../documents/mb6302__decontam.html') })
---


# 3 - decontaminate data (& check)

Filtering and trimming in the previous steps should have removed nearly all (`r emo::ji("crossed_fingers")`) of the _fake sequences_: low-quality bases that can not be trusted (trimming), as well as artificial sequences introduced by library-preparation and sequencing (filtering).

Next, it is a very good idea to consider what _real sequences_ have been included in our dataset, but do not belong in our analysis. Contaminants could come from: 

  * the experimental environment: host DNA, DNA from surfaces or substrates (foods?) in the environment
  * the method used: i.e. sampling methodology, lab equipment
  * the researcher: sneezing, dribbling, rhinotillexis, colds, coughs, sneezes, or deep PhD-related sighs
  
In some environments (e.g. faeces, sludge, soil), contaminating sequences might only comprise a small proportion of the total sequences, and are unlikely to change the "big picture" of the dataset, but could interfere with analysis of specific microbes, or metabolic functions in ways we cannot predict. In other environments (e.g. microbiomes from biopsy, the aeolian microbiome), contaminants can make up the majority of a sample, altering the "big picture" significantly, and making it difficult to draw meaningful conclusions - and you won't know why.

To address this as best we can, we remove contaminants based on a database of reference sequences **appropriate to our study**. We collect these sequences (search for the host, known contaminants, artificial sequences, foodstuffs, etc.), compile them into a database (i.e. a big file that's formatted so that it's easy for computers to search through it quickly), and then ask a program (e.g. the DNA-DNA alignment program, `bowtie2`) to look for, and remove, any sequences that match those contaminants.

For decontamination, we'll need the programs `BowTie2` (database-building & aligning) and `samtools` (filtering files in `sam` and `bam` formats), as well as FastQC & MultiQC for profiling the sequences output.

```{bash,eval=FALSE}
# make sure you're on a compute node!
# note the samtools version.. for the teagasc HPC at any rate.
module load bowtie2 samtools/1.10 parallel fastqc multiqc
```


## Decontaminate with `BowTie2`

We need reference genomes relevant to our experiment. For this tutorial, we consider several possible sources of host contamination:

  * [human genome, assembly `GRCh38.p14, GenBank GCF_000001405.40`](https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/) - contamination from the preparation, extraction, & sampling
  * [the "decoy genome", `hs37d5ss` ('11)](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/README_human_reference_20110707) - a bundle of synthetic, virus-associated, and other miscellaneous sequences that don't map well from/to the human genome ([see more jhere](http://www.cureffi.org/2013/02/01/the-decoy-genome/))
  * [the cow genome, GenBank `GCA_021234555.1` ('21, contig/scaffold N50s: 50M/104M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCA_021234555.1/) - from the source milk
  * [the sheep genome, GenBank `GCA_016772045.1` ('21, contig/scaffold N50s: 43M/101M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_016772045.1/) - from the source milk
  * [the goat genome, GenBank `GCA_001704415.2` ('16,  contig/scaffold N50s: 26M/87M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_001704415.2/) - from the source milk
  * [the chicken genome, genBank `GCA_016699485.1` ('21, contig/scaffold N50s: 19M/91M)](https://www.ncbi.nlm.nih.gov/labs/data-hub/genome/GCF_016699485.2/) - host for the chicken gut microbiome

We will first obtain all of these sequences from the NCBI (should/could use ENA...)

```{bash, eval = FALSE}
echo '#!/bin/bash

cd $1

# download your own genome (more or less) - 1.2 GB, 12min :: from the NCBI browser https://www.ncbi.nlm.nih.gov/data-hub/genome/GCF_000001405.40/
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_000001405.40/download?filename=GCF_000001405.40.zip" -H "Accept: application/zip"

# decoy genome - 9 MB
wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5ss.fa.gz

# cow genome - 825 MB, 5min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCA_021234555.1/download?filename=GCA_021234555.1.zip" -H "Accept: application/zip"

# sheep genome - 890 MB, 4min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016772045.1/download?filename=GCF_016772045.1.zip" -H "Accept: application/zip"

# goat genome - 930 MB, 3min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_001704415.2/download?filename=GCF_001704415.2.zip" -H "Accept: application/zip"

# chicken genome - 440 MB, 1min
curl -OJX GET "https://api.ncbi.nlm.nih.gov/datasets/v1/genome/accession/GCF_016699485.2/download?filename=GCF_016699485.2.zip" -H "Accept: application/zip"' > $MAT/trimmo_genomes_dl.sh

# do check that curl is installed!
# if not, try activating it: "module load curl"
curl -h

# give "run" permission for script to be run, for the user only
chmod u+x $MAT/trimmo_genomes_dl.sh
# make a dir for genomes in our RAW folder
mkdir $RAW/ref
# run the script while pointing at those files
$MAT/trimmo_genomes_dl.sh $RAW/ref 


```


Our genomes should be inside the downloaded `.zip` files, under the directory `/ncbi_dataset/data/<genbank-accession>/`, and will be the largest `file.fna` in there. Extract them to your `$RAW/ref` folder, and then compress the `.fna` to `.fna.gz` with `gzip` to save space. Genome data is fairly dense, so this will/might only compress to about 1/3 the raw size.

**NB**: check here that you have the genomes relevant to _you_, and make sure that the filenames above match the filenames used in the following step (e.g., does a file end in `.fna` or `.fa`?...)

```{bash, eval=FALSE}
parallel gzip {} ::: $RAW/ref/*fna

```


###  `BowTie2` : make database

We then need use `bowtie2` to build a reference database for all of these genomes in one place. `BT_threads=9` because available locally - feel free to increase to something appropriate to your setup, as we check the `decoy genome` first. 

```{bash, eval=FALSE}
# number of processes/threads to use for jobs
BT_threads=10

# the reference genomes (above) were downloaded here:
ls $RAW/ref

# the new bowtie2 databases will go here:
BT_DB=/data/Food/analysis/R0936_redcheese/ref
mkdir $BT_DB

# bowtie2-build [options]* <reference_in> <bt2_base>
time bowtie2-build --t $BT_threads $RAW/ref/hs37d5ss.fa.gz BT_DB/decoy_hs37d5ss.bt2 > $BT_DB/decoy_hs37d5ss.bt2.buildlog


## individual builds:
# decoy.9MB.fna.gz; 19 seconds for decoy, 9 threads, 15GB RAM
# chicken.340MB.fna.gz; 10min, 9 threads, 15GB RAM
# cow.853MB.fna.gz; 28m, 9 threads, 15GB RAM

```


That was quick! It was quick because the amount of data (`9MB`!) was small. Larger (real) genomes will require much longer to run, and likely require a `slurm` script.  

Although `jfg` is not certain of the theory behind aligning to an index built from _multiple_ reference genomes (probably it uses the first match, even if there are several paralogous sequences), it should be possible by listing several `fasta` files as input to `bowtie2-build file1.fna.gz,file2.fna.gz,file3.fna.gz...` etc (as per the [`bowtie2` manual](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#main-arguments-1)). This avoids having to `cat` all the `fasta` files together to make one giant `fasta`. 

We need to notify `slurm` of the requirements: 

  * as the human genome takes about a half hour to build, and there are 5-and-a-half genomes, we notify `slurm` that the job should take `30min * 5.5 ~= 3hr` for the genomes used here, with 10 processors and loads of memory. Alternatively, remove the `--time`` line to avoid the job being killed if it runs over. 
  * there are [suggestions](https://community.arm.com/arm-community-blogs/b/high-performance-computing-blog/posts/tuning-bowtie2-better-performance) that adding more and more threads to `bowtie2` doesn't make a huge difference, and it could mean we wait a long time for our job to be started. So we ask for only 10 threads. We don';'t use the `$BT_threads` variable this time, as `slurm` can't 'see' this variable when it starts.
  * Some desktop-pc tests have shown that 15GB of RAM is not enough for this job (>3GB per genome, x5.5...). RAM parameter is not specified below as the requirement is unknown.


```{bash, eval=FALSE}
# build our BT2 database in slurm:
echo '#!/bin/bash

#SBATCH --job-name=bt2DB
#SBATCH --output=bt2DB.txt
#SBATCH --ntasks=10
#SBATCH --time=179:00

INOUT=$1

# feel free to add / remove genomes that are relevant to you

# do check the names before you run! 
time bowtie2-build --large-index --threads 10 \
$INOUT/GCA_021234555.1_ARS-LIC_NZ_Jersey_genomic.fna.gz,\
$INOUT/GCF_000001405.40_GRCh38.p14_genomic.fna.gz,\
$INOUT/GCF_001704415.2_ARS1.2_genomic.fna.gz,\
$INOUT/GCF_016699485.2_bGalGal1.mat.broiler.GRCg7b_genomic.fna.gz,\
$INOUT/GCF_016772045.1_ARS-UI_Ramb_v2.0_genomic.fna.gz,\
$INOUT/hs37d5ss.fa.gz \
$INOUT/mult_ref #> $INOUT/mult_ref.buildlog # 2>&1

' > $MAT/slurm_bt2db.sh

# and then run
$MAT/slurm_bt2db.sh $BT_DB

```

This will give us a number of files ending in `.bt2` in our output dir. These are the reference files that `bowtie2` uses to check for contaminant sequences in the `fastq` data (if you want, you can now delete the reference `fna.gz` genomes!).


### `BowTie2` : find contaminants 

[`Bowtie2` will align our dataset](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#introduction) against these reference indices, and make a note of what reads matched between the input (dataset) and reference (host). For our study, we will be removing any reads that match the reference (hosts) as we are looking at the microbiome specifically. However, in other studies, we might be interested in the host sequences only, and would throw away everything else (all the microbes). 

**NB**: bowtie2 output interpretation

**NB**: ths SAM tools spec



```{bash, eval=FALSE}
## firstly, align $TEST sequences to ref using bt2.
bowtie2 -p $BT_threads -x $BT_DB/mult_ref -1 $FILT/${TEST}_R1_trimm.fastq.gz -2 $FILT/${TEST}_R2_trimm.fastq.gz -S $KDOUT/${TEST}_bt2_refmapped.sam

## feel free to look at the output
less $KDOUT/${TEST}_bt2_refmapped.sam
```

We _can_ use `BowTie2` directly for filtering out contaminant reads, but because there are paired (`F+R`) reads, it's not quite as specific as we'd like. For better control, we can use the `samtools` program below.


### `samtools` : delete contaminants

Now that `Bowtie2` has identified the contaminant sequences, we use `samtools` to:

  1. convert the `sam` file to a `bam` file
  2. filter out any reads assigned to the decontamination database
  3. sort all the remaining reads, so that `F+R` are in the same order
  4. convert the `bam` file to a fastq (throwing the extra sam info into the `/dev/null` bin)
  

```{bash, eval=FALSE}
## 1 - view with samtools, and convert to bam (binary)
samtools view -bS $KDOUT/${TEST}_bt2_refmapped.sam > $KDOUT/${TEST}_bt2_refmapped.bam

## 2 - filter the contaminant sequences out
# F 256 : exclude those mapping to ref
# f 12 : F and R *both* unmapped
samtools view -b -f 12 -F 256 $KDOUT/${TEST}_bt2_refmapped.bam > $KDOUT/${TEST}_bt2_decontamd.bam

## 3 = ensure reads are in a sensible order 
samtools sort -n -m 5G -@ 2 $KDOUT/${TEST}_bt2_decontamd.bam -o $KDOUT/${TEST}_bt2_decontamd_sort.bam > ${TEST}_bam_sort.out

## 4 - stream outputs of non-interest to /dev/null
samtools fastq -@ 8 $KDOUT/${TEST}_bt2_decontamd_sort.bam -1 $KDOUT/${TEST}_bt2decon_R1.fastq.gz -2 $KDOUT/${TEST}_bt2decon_R2.fastq.gz -0 /dev/null -s /dev/null -n

# look at the fastq! 
ls -lsh $KDOUT/${TEST}*

## removing intermediates: SAM and BAM files are huge! delete --WHEN FINISHED--
rm $KDOUT/${TEST}*[sam,bam]

```


### decontaminate multiple samples

We can apply the same technique to all of our samples using `slurm`. The database is already present, so we write `slurm` scripts for the `BowTie2` alignment, `samtools` filtering, and `samtools` processing.

Separating out the jobs allows us to troubleshoot issues more easily (hopefully), but complicates batch processing of samples. It is more efficient, and possibly easier to supervise, if each sample was processed together, and errors produce a simple message that we can check. Here, we use `if` statements to check that the outputs are finished, before we delete all the intermediate `bam` and `sam` files (which get very large).

```
**NB-jfg** : `time` flag is totally spurious ; lots of large intermediate files being created - we'll need to delete these as we go;  **see the `bt_deco.txt` for output**, which should be in the `~` directory... Problems may be recorded there ; also check name formats in bt2 align step; issue with streaming applications sending binary to .out/.log files - logfile call deleted in repsonse ; need slurm individual name tag
```

```{bash, eval=FALSE}

echo '#!/bin/bash

#SBATCH --job-name=deco_bt2.%j
#SBATCH --output=deco_bt2.%j.txt
#SBATCH --ntasks=10

SAMPLE=$1
IN=$2
OUT=$3
REF=$4
THREADS=$5


# load necess
module load bowtie2 samtools/1.10


## stamp for the output
DAT_TIM=$(date +"%T %F")
echo "--------------------------------------------"
echo $DAT_TIM
echo sampl: $SAMPLE
echo input: $IN
echo outpt: $OUT
echo "--------------------------------------------"


echo "--   align   -------------------------------------"
# -x $REF/mult_ref \
time bowtie2 -p $THREADS \
-x $REF/mult_ref \
-1 $IN/${SAMPLE}_R1_trimm.fastq.gz \
-2 $IN/${SAMPLE}_R2_trimm.fastq.gz \
-S $OUT/${SAMPLE}_bt2_refmapped.sam


if [ $? = 0 ] && [ -s $OUT/${SAMPLE}_bt2_refmapped.sam ]
then
        echo "--   change format   -------------------------------------"
        time samtools view -bS $OUT/${SAMPLE}_bt2_refmapped.sam > $OUT/${SAMPLE}_bt2_refmapped.bam


        echo "--   remove matches   ------------------------------------"
        time samtools view -b -f 12 -F 256 $OUT/${SAMPLE}_bt2_refmapped.bam > $OUT/${SAMPLE}_bt2_decontamd.bam


        ## obviate - not enough memory, and not essential for kraken & co.
        # echo "--   re-organise reads   ---------------------------------"
        # time samtools sort -n -m 5G -@ $THREADS $OUT/${SAMPLE}_bt2_decontamd.bam -o $OUT/${SAMPLE}_bt2_decontamd_sort.bam


        echo "--   to fastq, dump other   ------------------------------"
        time samtools fastq -@ $THREADS $OUT/${SAMPLE}_bt2_decontamd_sort.bam -1 $OUT/${SAMPLE}_bt2decon_R1.fastq.gz -2 $OUT/${SAMPLE}_bt2decon_R2.fastq.gz -0 /dev/null -s /dev/null -n

        echo "---   $SAMPLE completed  :)   ----------------------------------"


else
        echo ""
        echo "!!!---   align failed   ::   $SAMPLE   ---!!!"
        echo ""
        echo ""

fi


## shrink intermediates
if [ -s $OUT/${SAMPLE}_bt2decon_R1.fastq.gz ] && [ -s $OUT/${SAMPLE}_bt2decon_R2.fastq.gz ] ;
then 
        echo "--   shrinking *AMs   ----------------------------------" ;
        gzip $OUT/${SAMPLE}_*.{bam,sam} &
fi

' > $MAT/slurm_deco_full.sh # SAMPLE  IN  OUT REF THREADS


# send just one sample to the script with "head -1"
for i in $( cat $MAT/samples | head -1 );
    do sbatch $MAT/slurm_deco_full.sh $i $FILT $KDOUT $BT_DB 10 ;
done

```


Often some samples don't work, e.g. due to our database being too big (`bowtie2` error code 11: SEGV). We can make a file named `bad_samples` that lists the samples that didn't work, and re-submit them with fewer threads. This seems to work fine, but if not consider checking your error messages in (`deco_bt2.####.txt`). 

```{bash,eval=FALSE}
## feel free to make up some samples 
echo -e "XXXX\nYYYY\nZZZ\nWWW" > $MAT/bad_samples


# delete the broken pieces of samples that didnt finish
for i in $( cat $MAT/bad_samples );
    do rm $KDOUT/${i}* ;
done


## try fix bad_samples
for i in $( cat $MAT/bad_samples );
    do sbatch $MAT/slurm_deco_full.sh $i $FILT $KDOUT $BT_DB 4 ;
done


# check outputs, check error messages

```




### Alternatively: decontaminate with `KneadData`

`Kneaddata` is a quality control pipeline developed by the [Huttenhower group](https://huttenhower.sph.harvard.edu) (home of `HUMAnN*`, `MetaPhlan*`, `LEfSe`, etc.) to do all of the above steps: `trimmomatic`, `bowtie2`, and `samtools`, in one go, using the human genome as a default reference. It can be very useful if we're setting things up for the first time, as it will download the required programs and a human database for us. We did not use it here because it is incredibly useful to have some idea of how to use `trimmomatic` and `bowtie2` to refine & identify reads of interest, alongside the face that our host sequences were not limited to humans and our programs are already installed.

If we wanted to achieve the same things with `KneadData`, we could provide similar arguments to those above (often, its the _exact_ same argument, we're just asking `KneadData` to tell `Trimmomatic`/`BowTie2`). Note that we would still have had to download our reference genomes and build a database from them using `bowtie-build`. 


## Microbiome Analysis

  * <a href="mb6302__setup.html">`workspace setup`</a>
  * <a href="mb6302__qc.html">`sequence QC`</a>
  * <a href="mb6302__decontam.html">`sequence decontamination`</a>
  * <a href="mb6302__taxonomy.html">`sequence taxonomic estimation`</a>

